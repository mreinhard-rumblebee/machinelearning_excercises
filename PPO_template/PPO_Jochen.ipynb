{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import gym\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Gym environment\n",
    "class GymEnvironment:\n",
    "    def __init__(self, env_id, monitor_dir, max_timesteps=400):\n",
    "        self.max_timesteps = max_timesteps\n",
    "\n",
    "        self.env = gym.make(env_id)\n",
    "\n",
    "environment = GymEnvironment('CartPole-v0', 'gymresults/cartpole-v0')\n",
    "env = environment.env"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "next_state, reward, done, _ = env.step(1)\n",
    "type(next_state.reshape(1,4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next_state.reshape(1, env.observation_space.shape[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n      dtype=float32)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.env.observation_space.low"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions, states = [], []\n",
    "states"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# simulate random action\n",
    "import time\n",
    "num_steps = 500\n",
    "obs = environment.env.reset()\n",
    "\n",
    "for step in range(num_steps):\n",
    "    action = environment.env.action_space.sample()\n",
    "    obs, reward, done, _ = environment.env.step(action)\n",
    "    environment.env.render()\n",
    "    time.sleep(0.01)\n",
    "    if done:\n",
    "        environment.env.reset()\n",
    "\n",
    "environment.env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# define Neural Networks\n",
    "class critic(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.d1 = tf.keras.layers.Dense(128,activation='relu')\n",
    "    self.v = tf.keras.layers.Dense(1, activation = None)\n",
    "\n",
    "  def call(self, input_data):\n",
    "    x = self.d1(input_data)\n",
    "    v = self.v(x)\n",
    "    return v\n",
    "\n",
    "\n",
    "class actor(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.d1 = tf.keras.layers.Dense(128,activation='relu')\n",
    "    self.a = tf.keras.layers.Dense(2,activation='softmax')\n",
    "\n",
    "  def call(self, input_data):\n",
    "    x = self.d1(input_data)\n",
    "    a = self.a(x)\n",
    "    return a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# action selection\n",
    "class agent():\n",
    "    def __init__(self):\n",
    "        self.a_opt = tf.keras.optimizers.Adam(learning_rate=7e-3)\n",
    "        self.c_opt = tf.keras.optimizers.Adam(learning_rate=7e-3)\n",
    "        self.actor = actor()\n",
    "        self.critic = critic()\n",
    "        self.clip_pram = 0.2\n",
    "\n",
    "\n",
    "    def act(self,state):\n",
    "        prob = self.actor(np.array([state]))\n",
    "        prob = prob.numpy()\n",
    "        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
    "        action = dist.sample()\n",
    "        return int(action.numpy()[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "action = agentoo7.act(state)\n",
    "action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "return 0 if logit[0][0] > logit[0][1] else return 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# test model knowledge\n",
    "def test_reward(env):\n",
    "  total_reward = 0\n",
    "  state = env.reset()\n",
    "  done = False\n",
    "  while not done:\n",
    "    action = np.argmax(agentoo7.actor(np.array([state])).numpy())\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "\n",
    "  return total_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new episod\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'agent' object has no attribute 'learn'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 12>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     50\u001B[0m states, actions,returns, adv  \u001B[38;5;241m=\u001B[39m preprocess1(states, actions, rewards, dones, values, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epocs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m---> 53\u001B[0m     al,cl \u001B[38;5;241m=\u001B[39m \u001B[43magentoo7\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m(states, actions, adv, probs, returns)\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;66;03m# print(f\"al{al}\")\u001B[39;00m\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;66;03m# print(f\"cl{cl}\")\u001B[39;00m\n\u001B[0;32m     57\u001B[0m avg_reward \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean([test_reward(env) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m5\u001B[39m)])\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'agent' object has no attribute 'learn'"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "tf.random.set_seed(336699)\n",
    "agentoo7 = agent()\n",
    "steps = 50\n",
    "ep_reward = []\n",
    "total_avgr = []\n",
    "target = False\n",
    "best_reward = 0\n",
    "avg_rewards_list = []\n",
    "\n",
    "\n",
    "for s in range(steps):\n",
    "  if target == True:\n",
    "          break\n",
    "\n",
    "  done = False\n",
    "  state = env.reset()\n",
    "  all_aloss = []\n",
    "  all_closs = []\n",
    "  rewards = []\n",
    "  states = []\n",
    "  actions = []\n",
    "  probs = []\n",
    "  dones = []\n",
    "  values = []\n",
    "  print(\"new episod\")\n",
    "\n",
    "  for e in range(128):\n",
    "\n",
    "    action = agentoo7.act(state)\n",
    "    value = agentoo7.critic(np.array([state])).numpy()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    dones.append(1-done)\n",
    "    rewards.append(reward)\n",
    "    states.append(state)\n",
    "    #actions.append(tf.one_hot(action, 2, dtype=tf.int32).numpy().tolist())\n",
    "    actions.append(action)\n",
    "    prob = agentoo7.actor(np.array([state]))\n",
    "    probs.append(prob[0])\n",
    "    values.append(value[0][0])\n",
    "    state = next_state\n",
    "    if done:\n",
    "      env.reset()\n",
    "\n",
    "  value = agentoo7.critic(np.array([state])).numpy()\n",
    "  values.append(value[0][0])\n",
    "  np.reshape(probs, (len(probs),2))\n",
    "  probs = np.stack(probs, axis=0)\n",
    "\n",
    "  states, actions,returns, adv  = preprocess1(states, actions, rewards, dones, values, 1)\n",
    "\n",
    "  for epocs in range(10):\n",
    "      al,cl = agentoo7.learn(states, actions, adv, probs, returns)\n",
    "      # print(f\"al{al}\")\n",
    "      # print(f\"cl{cl}\")\n",
    "\n",
    "  avg_reward = np.mean([test_reward(env) for _ in range(5)])\n",
    "  print(f\"total test reward is {avg_reward}\")\n",
    "  avg_rewards_list.append(avg_reward)\n",
    "  if avg_reward > best_reward:\n",
    "        print('best reward=' + str(avg_reward))\n",
    "        agentoo7.actor.save('model_actor_{}_{}'.format(s, avg_reward), save_format=\"tf\")\n",
    "        agentoo7.critic.save('model_critic_{}_{}'.format(s, avg_reward), save_format=\"tf\")\n",
    "        best_reward = avg_reward\n",
    "  if best_reward == 200:\n",
    "        target = True\n",
    "  env.reset()\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Generalized Advantage Estimation\n",
    "def preprocess1(states, actions, rewards, done, values, gamma):\n",
    "    g = 0\n",
    "    lmbda = 0.95\n",
    "    returns = []\n",
    "    for i in reversed(range(len(rewards))):\n",
    "       delta = rewards[i] + gamma * values[i + 1] * done[i] - values[i]\n",
    "       g = delta + gamma * lmbda * dones[i] * g\n",
    "       returns.append(g + values[i])\n",
    "\n",
    "    returns.reverse()\n",
    "    adv = np.array(returns, dtype=np.float32) - values[:-1]\n",
    "    adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
    "    states = np.array(states, dtype=np.float32)\n",
    "    actions = np.array(actions, dtype=np.int32)\n",
    "    returns = np.array(returns, dtype=np.float32)\n",
    "    return states, actions, returns, adv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Learning function\n",
    "def learn(self, states, actions,  adv , old_probs, discnt_rewards):\n",
    "        discnt_rewards = tf.reshape(discnt_rewards, (len(discnt_rewards),))\n",
    "        adv = tf.reshape(adv, (len(adv),))\n",
    "\n",
    "        old_p = old_probs\n",
    "\n",
    "        old_p = tf.reshape(old_p, (len(old_p),2))\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            p = self.actor(states, training=True)\n",
    "            v =  self.critic(states,training=True)\n",
    "            v = tf.reshape(v, (len(v),))\n",
    "            td = tf.math.subtract(discnt_rewards, v)\n",
    "            c_loss = 0.5 * kls.mean_squared_error(discnt_rewards, v)\n",
    "            a_loss = self.actor_loss(p, actions, adv, old_probs, c_loss)\n",
    "\n",
    "        grads1 = tape1.gradient(a_loss, self.actor.trainable_variables)\n",
    "        grads2 = tape2.gradient(c_loss, self.critic.trainable_variables)\n",
    "        self.a_opt.apply_gradients(zip(grads1, self.actor.trainable_variables))\n",
    "        self.c_opt.apply_gradients(zip(grads2, self.critic.trainable_variables))\n",
    "        return a_loss, c_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Actor loss\n",
    "def actor_loss(self, probs, actions, adv, old_probs, closs):\n",
    "\n",
    "        probability = probs\n",
    "        entropy = tf.reduce_mean(tf.math.negative(tf.math.multiply(probability,tf.math.log(probability))))\n",
    "        #print(probability)\n",
    "        #print(entropy)\n",
    "        sur1 = []\n",
    "        sur2 = []\n",
    "\n",
    "        for pb, t, op in zip(probability, adv, old_probs):\n",
    "                        t =  tf.constant(t)\n",
    "                        op =  tf.constant(op)\n",
    "                        #print(f\"t{t}\")\n",
    "                        #ratio = tf.math.exp(tf.math.log(pb + 1e-10) - tf.math.log(op + 1e-10))\n",
    "                        ratio = tf.math.divide(pb,op)\n",
    "                        #print(f\"ratio{ratio}\")\n",
    "                        s1 = tf.math.multiply(ratio,t)\n",
    "                        #print(f\"s1{s1}\")\n",
    "                        s2 =  tf.math.multiply(tf.clip_by_value(ratio, 1.0 - self.clip_pram, 1.0 + self.clip_pram),t)\n",
    "                        #print(f\"s2{s2}\")\n",
    "                        sur1.append(s1)\n",
    "                        sur2.append(s2)\n",
    "\n",
    "        sr1 = tf.stack(sur1)\n",
    "        sr2 = tf.stack(sur2)\n",
    "\n",
    "        #closs = tf.reduce_mean(tf.math.square(td))\n",
    "        loss = tf.math.negative(tf.reduce_mean(tf.math.minimum(sr1, sr2)) - closs + 0.001 * entropy)\n",
    "        #print(loss)\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-ml_exam-py",
   "language": "python",
   "display_name": "Python [conda env:ml_exam] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}