{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import gym\n",
    "import scipy.signal\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    #print(f'G_lam: {scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]}')\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        #print(f'observations: {type(self.observation_buffer)}, {len(self.observation_buffer)}, {self.observation_buffer}')\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        #print(f'path_slice: {path_slice}')\n",
    "        #print(f'reward_buffer: {self.reward_buffer}')\n",
    "        #print(f'reward_buffer_slice: {self.reward_buffer[path_slice]}')\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        #print(f'rewards: {type(rewards)}, {rewards}')\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        #print(f'values: {type(self.value_buffer)}, {len(self.value_buffer)}, {self.value_buffer}')\n",
    "        #print(f'G_lams: {type(self.return_buffer)}, {len(self.return_buffer)}, {self.return_buffer}')\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "def mlp(x, sizes, activation=tf.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "    for size in sizes[:-1]:\n",
    "        x = layers.Dense(units=size, activation=activation)(x)\n",
    "    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    #print(f'logits: {logits}')\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    #print(f'logprob: {logprobability}')\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        #print(f'ratio: {type(ratio)}, {len(ratio)}, {ratio}')\n",
    "        #print(f'advantages: {type(min_advantage)}, {len(min_advantage)}, {min_advantage}')\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = tf.reduce_mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = tf.reduce_sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    #print(f'observations: {type(observation_buffer)}, {observation_buffer}')\n",
    "    #print(f'values: {type(critic(observation_buffer))}, {critic(observation_buffer)}')\n",
    "    #print(f'returns: {type(return_buffer)}, {return_buffer}')\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    #print(f'value_loss: {value_loss}')\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    #print(f'value_grads: {value_grads}')\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Hyperparameters of the PPO algorithm\n",
    "steps_per_epoch = 100 #10\n",
    "epochs = 300 #30\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 3e-4\n",
    "value_function_learning_rate = 1e-3\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "lam = 0.97\n",
    "target_kl = 0.01\n",
    "hidden_sizes = (64, 64)\n",
    "\n",
    "# True if you want to render the environment\n",
    "render = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Initialize the environment and get the dimensionality of the\n",
    "# observation space and the number of possible actions\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Initialize the buffer\n",
    "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "\n",
    "# Initialize the actor and the critic as keras models\n",
    "observation_input = keras.Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "logits = mlp(observation_input, list(hidden_sizes) + [num_actions], tf.tanh, None)\n",
    "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "value = tf.squeeze(\n",
    "    mlp(observation_input, list(hidden_sizes) + [1], tf.tanh, None), axis=1\n",
    ")\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "# Initialize the policy and the value function optimizers\n",
    "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, episode_return, episode_length = env.reset(), 0, 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: <class 'tensorflow.python.framework.ops.Tensor'>, 100, Tensor(\"Exp:0\", shape=(100,), dtype=float32)\n",
      "advantages: <class 'tensorflow.python.framework.ops.Tensor'>, 100, Tensor(\"SelectV2:0\", shape=(100,), dtype=float32)\n",
      "ratio: <class 'tensorflow.python.framework.ops.Tensor'>, 100, Tensor(\"Exp:0\", shape=(100,), dtype=float32)\n",
      "advantages: <class 'tensorflow.python.framework.ops.Tensor'>, 100, Tensor(\"SelectV2:0\", shape=(100,), dtype=float32)\n",
      " Epoch: 1. Mean Return: 20.0. Mean Length: 20.0\n",
      " Epoch: 2. Mean Return: 20.0. Mean Length: 20.0\n",
      " Epoch: 3. Mean Return: 33.333333333333336. Mean Length: 33.333333333333336\n",
      " Epoch: 4. Mean Return: 33.333333333333336. Mean Length: 33.333333333333336\n",
      " Epoch: 5. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 6. Mean Return: 33.333333333333336. Mean Length: 33.333333333333336\n",
      " Epoch: 7. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 8. Mean Return: 33.333333333333336. Mean Length: 33.333333333333336\n",
      " Epoch: 9. Mean Return: 33.333333333333336. Mean Length: 33.333333333333336\n",
      " Epoch: 10. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 11. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 12. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 13. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 14. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 15. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 16. Mean Return: 33.333333333333336. Mean Length: 33.333333333333336\n",
      " Epoch: 17. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 18. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 19. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 20. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 21. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 22. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 23. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 24. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 25. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 26. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 27. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 28. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 29. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 30. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 31. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 32. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 33. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 34. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 35. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 36. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 37. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 38. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 39. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 40. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 41. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 42. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 43. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 44. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 45. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 46. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 47. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 48. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 49. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 50. Mean Return: 33.333333333333336. Mean Length: 33.333333333333336\n",
      " Epoch: 51. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 52. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 53. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 54. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 55. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 56. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 57. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 58. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 59. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 60. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 61. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 62. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 63. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 64. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 65. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 66. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 67. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 68. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 69. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 70. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 71. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 72. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 73. Mean Return: 50.0. Mean Length: 50.0\n",
      " Epoch: 74. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 75. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 76. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 77. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 78. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 79. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 80. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 81. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 82. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 83. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 84. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 85. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 86. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 87. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 88. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 89. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 90. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 91. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 92. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 93. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 94. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 95. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 96. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 97. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 98. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 99. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 100. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 101. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 102. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 103. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 104. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 105. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 106. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 107. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 108. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 109. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 110. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 111. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 112. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 113. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 114. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 115. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 116. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 117. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 118. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 119. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 120. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 121. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 122. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 123. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 124. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 125. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 126. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 127. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 128. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 129. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 130. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 131. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 132. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 133. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 134. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 135. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 136. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 137. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 138. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 139. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 140. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 141. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 142. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 143. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 144. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 145. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 146. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 147. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 148. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 149. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 150. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 151. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 152. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 153. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 154. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 155. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 156. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 157. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 158. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 159. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 160. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 161. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 162. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 163. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 164. Mean Return: 100.0. Mean Length: 100.0\n",
      " Epoch: 165. Mean Return: 100.0. Mean Length: 100.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [21]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;66;03m# Update the value function\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(train_value_iterations):\n\u001B[0;32m---> 64\u001B[0m     \u001B[43mtrain_value_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_buffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;66;03m# Print mean return and length for each epoch\u001B[39;00m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Epoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Mean Return: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msum_return \u001B[38;5;241m/\u001B[39m num_episodes\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Mean Length: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msum_length \u001B[38;5;241m/\u001B[39m num_episodes\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     69\u001B[0m )\n",
      "File \u001B[0;32m~/Documents/Dokumente-Heiko/E-UNI/FIM/ML/machinelearning_excercises/venv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/Documents/Dokumente-Heiko/E-UNI/FIM/ML/machinelearning_excercises/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:910\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    907\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    909\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 910\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    912\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    913\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/Documents/Dokumente-Heiko/E-UNI/FIM/ML/machinelearning_excercises/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:942\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    939\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    940\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    941\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 942\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stateless_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    943\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    944\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    945\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    946\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/Documents/Dokumente-Heiko/E-UNI/FIM/ML/machinelearning_excercises/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3130\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3127\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m   3128\u001B[0m   (graph_function,\n\u001B[1;32m   3129\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m-> 3130\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3131\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Dokumente-Heiko/E-UNI/FIM/ML/machinelearning_excercises/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1959\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1955\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1956\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1957\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1958\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1959\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1960\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1961\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1962\u001B[0m     args,\n\u001B[1;32m   1963\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1964\u001B[0m     executing_eagerly)\n\u001B[1;32m   1965\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/Documents/Dokumente-Heiko/E-UNI/FIM/ML/machinelearning_excercises/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py:598\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    596\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    597\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 598\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    599\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    600\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    601\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    602\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    604\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    605\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m    606\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[1;32m    607\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    610\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[1;32m    611\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[0;32m~/Documents/Dokumente-Heiko/E-UNI/FIM/ML/machinelearning_excercises/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:58\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     57\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 58\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     61\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate over the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "\n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in range(steps_per_epoch):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        observation = observation.reshape(1, -1)\n",
    "        logits, action = sample_action(observation)\n",
    "        observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        # Get the value and log-probability of the action\n",
    "        #print(f'observation: {type(observation)}, {observation}')\n",
    "        value_t = critic(observation)\n",
    "        #print(f'value_t: {type(value_t)}, {value_t}')\n",
    "        logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        buffer.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "        # Update the observation\n",
    "        observation = observation_new\n",
    "\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "            buffer.finish_trajectory(last_value)\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "\n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = buffer.get()\n",
    "\n",
    "    #print(f'test: {logprobabilities(actor(observation_buffer), action_buffer)- logprobability_buffer}')\n",
    "\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    # Print mean return and length for each epoch\n",
    "    print(\n",
    "        f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]], dtype=float32)"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(actor.trainable_variables)\n",
    "np.zeros((10,4), dtype=np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "\n",
    "G_lam = [7.62, 7.02, 6.43, 5.79]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "machinelearning_exercises",
   "language": "python",
   "display_name": "machinelearning_exercises"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}