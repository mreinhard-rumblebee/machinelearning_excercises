{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import gym\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class GymEnvironment:\n",
    "    def __init__(self, env_id, monitor_dir, max_timesteps=400): #default max_timesteps = 400, testing: 10\n",
    "        self.max_timesteps = max_timesteps\n",
    "\n",
    "        self.env = gym.make(env_id)\n",
    "\n",
    "    def trainPPO(self, agent, no_episodes):\n",
    "        self.runPPO(agent, no_episodes, training=True)\n",
    "\n",
    "    def runPPO(self, agent, no_episodes, training=False):\n",
    "\n",
    "        rew = []\n",
    "        for episode in range(no_episodes):\n",
    "            #state = self.env.reset().reshape(1, self.env.observation_space.shape[0])\n",
    "            states = np.zeros((agent.actors*self.max_timesteps, agent.state_size), dtype = np.float32)\n",
    "            actions = np.zeros(agent.actors*self.max_timesteps, dtype = np.int32)\n",
    "            logprobs = np.zeros(agent.actors*self.max_timesteps, dtype = np.float32)\n",
    "            advantages = []\n",
    "            G_lams = []\n",
    "            #values_global = []\n",
    "            storage_counter = 0\n",
    "            #states, actions, G_lams, values_global, logprobs = [], [], [], [], []\n",
    "\n",
    "            for n in range(0, agent.actors):\n",
    "                state = self.env.reset().reshape(1, self.env.observation_space.shape[0])\n",
    "                # Problem: if done==1, the pole will always fall, so a reset might be good ???\n",
    "                tot_rew = 0\n",
    "                values = np.zeros(self.max_timesteps)\n",
    "                rewards = np.zeros(self.max_timesteps+1)\n",
    "                #values, rewards = [], []\n",
    "                for t in range(self.max_timesteps):\n",
    "                    # TODO: Fill out the respective to-dos in this loop and make sure that the overall algorithm works,\n",
    "                    #  e.g., overwrite current state with next state entering a new time step\n",
    "\n",
    "                    logit, action = agent.select_action(state)\n",
    "                    next_state, reward, done, _ = self.env.step(action.numpy()[0])\n",
    "\n",
    "                    next_state = next_state.reshape(1, self.env.observation_space.shape[0])\n",
    "                    tot_rew += reward\n",
    "\n",
    "                    states[storage_counter] = state\n",
    "                    actions[storage_counter] = action\n",
    "\n",
    "                    \"\"\"\n",
    "                    if (done == True) or (t == self.max_timesteps-1):\n",
    "                        #checking performance:\n",
    "                        print(f'Episode {episode} actor {n}: reward: {tot_rew} /{self.max_timesteps}')\n",
    "                    \"\"\"\n",
    "\n",
    "                    #print(f'state: {type(state)}, {state}')\n",
    "                    if training == True:\n",
    "                        # TODO: Store relevant transition information such as rewards, values, etc. that you will need in\n",
    "                        #  the calculation of the advantages later\n",
    "                        rewards[t] = reward\n",
    "                        #values.append(agent.critic.predict(state)[0])\n",
    "                        values[t] = agent.critic(state)[0]\n",
    "                        #print(f'value: {type(agent.critic(state)[0])}, {agent.critic(state)[0]}')\n",
    "                        logprobs[storage_counter] = policy_probabilities(logit, action)\n",
    "\n",
    "                    state = next_state\n",
    "                    storage_counter += 1\n",
    "\n",
    "                    if ((done == True) or (t == self.max_timesteps-1)) and (training == True):\n",
    "                        # Calculate advantages when the function breaks or the last iteration is reached\n",
    "                        # TODO: Call function for calculation and storage of advantages\n",
    "                        rewards = rewards[:t+1]\n",
    "                        values = values[:t+1]\n",
    "                        deltas = rewards + agent.gamma * values - values\n",
    "                        advantages = np.append(advantages, discounted_cumulative_sums(deltas, agent.gamma*agent.lam))\n",
    "                        if done == True:\n",
    "                            rewards = np.array(np.append(rewards, 0))\n",
    "                        else:\n",
    "                            rewards = np.array(np.append(rewards, agent.critic(state)[0]))\n",
    "                        G_lams = np.append(G_lams, discounted_cumulative_sums(rewards, agent.gamma))[:-1]\n",
    "                        #G_lams = np.append(G_lams, agent.calc_advantage(values, rewards, t+1))\n",
    "                        # TODO: Store targets for your value function update\n",
    "                        #values_global = np.append(values_global, values)\n",
    "                        #print(f'rewards: {type(rewards)}, {rewards}')\n",
    "                        print(f'Episode {episode} actor {n}: reward: {tot_rew} /{self.max_timesteps}')\n",
    "                        break\n",
    "\n",
    "                rew.append(tot_rew)\n",
    "\n",
    "            # TODO: If training, Call function to update value function weights\n",
    "            #print(f'len_states: {len(states)}')\n",
    "            #print(f'critic_global: {agent.critic(states)}')\n",
    "            #print(f'values_global: {type(values_global)}, {len(values_global)} {values_global}')\n",
    "            #print(f'G_lams: {type(G_lams)}, {len(G_lams)}, {G_lams}')\n",
    "            #print(f'counter: {storage_counter}')\n",
    "            #agent.update_value_parameters(G_lams, values_global)\n",
    "            if training == True:\n",
    "                G_lams = np.array(G_lams)\n",
    "                G_lams = G_lams.astype(dtype = np.float32)\n",
    "                advantages = np.array(advantages)\n",
    "                advantages = advantages.astype(dtype = np.float32)\n",
    "                advantages = (advantages - np.mean(advantages)) / np.std(advantages)\n",
    "                #print(f'G_lams1: {type(G_lams)}, {len(G_lams)}, {G_lams}')\n",
    "                states = states[0:len(G_lams)]\n",
    "                actions = actions[0:len(G_lams)]\n",
    "                logprobs = logprobs[0:len(G_lams)]\n",
    "                values_global = agent.critic(states)[0]\n",
    "                #advantages = np.subtract(G_lams, values_global)\n",
    "                #print(f'advantages1: {type(advantages)}, {len(advantages)}, {advantages}')\n",
    "                #print(f'states: {type(states)}, {len(states)}, {states}')\n",
    "                # TODO: If training, call function to update policy function weights using clipping\n",
    "                agent.update_policy_parameters(states,actions,logprobs,G_lams,advantages)\n",
    "                #print(f'test: {policy_probabilities(agent.actor(states), actions) - logprobs}')\n",
    "                # TODO: If training, Call function to update value function weights\n",
    "                agent.update_value_parameters(G_lams, states)\n",
    "                #agent.update_value_parameters(G_lams,values_global)\n",
    "                # TODO: Implement here a function that evaulates the agent's performance for every x episodes by\n",
    "                #  calling PPO directly and returns an average of total rewards for 100 runs, if your objective is\n",
    "                #  reached, you can terminate training\n",
    "\n",
    "        print(f'total reward: {sum(rew)} /{self.max_timesteps*agent.actors*no_episodes}')\n",
    "        return rew"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def policy_probabilities(logit, action):\n",
    "    # TODO: Compute probabilities of taking actions a by using the outputs of actor NN (the logits)\n",
    "    # softmax calculation\n",
    "    #logprobs = tf.math.log(np.exp(logit) / np.sum(np.exp(logit)))\n",
    "    #print(f'logits: {logit}')\n",
    "    logprobs = tf.nn.log_softmax(logit)\n",
    "    #action = tf.dtype.cast(action, dtype = float)\n",
    "    logprob = tf.reduce_sum(tf.one_hot(action, 2) * logprobs, axis=1)\n",
    "    #print(f'logprob: {logprob}')\n",
    "    return logprob\n",
    "\n",
    "\n",
    "# Sum of discountated rewards of vectors --> useful for advantage estimates or total rewards\n",
    "def discounted_cumulative_sums(x, discount):\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class PPO_Agent:\n",
    "    def __init__(self, no_of_states, no_of_actions):\n",
    "        self.state_size = no_of_states\n",
    "        self.action_size = no_of_actions\n",
    "\n",
    "        # TODO: Set hyperparameters and vary them\n",
    "        self.gamma = 0.99  # discount rate, 0.9\n",
    "        self.lam = 0.97  # lambda for TD(lambda), 0.6\n",
    "        self.clip_ratio = 0.2  # Clipping ratio for calculating L_clip, 0.5\n",
    "        self.lr = 0.0003  # learning rate, default: 0.0001\n",
    "        self.actors = 10  # Number of parallel actors, default: 100, testing: 10\n",
    "\n",
    "        self.policy_iterations = 20 # number of policy updates\n",
    "        self.value_iterations = 20 # number of value updates\n",
    "\n",
    "        # self.actor = actor()\n",
    "        # self.critic = critic()\n",
    "        self.actor = self.nn_model(self.state_size, self.action_size)\n",
    "        self.actor_optimizer = keras.optimizers.Adam(learning_rate=self.lr)\n",
    "        self.critic = self.nn_model(self.state_size, 1)\n",
    "        self.critic_optimizer = keras.optimizers.Adam(learning_rate=self.lr)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # TODO: Implement action selection, i.e., sample an action from policy pi\n",
    "        # logit = self.actor(np.array([state])).numpy()\n",
    "        #logit = self.actor(state).numpy()\n",
    "        logit = agent.actor(state)\n",
    "        action = tf.squeeze(tf.random.categorical(logit, 1), axis=1)\n",
    "        return logit, action\n",
    "\n",
    "    def calc_advantage(self, values, rew, T):\n",
    "        # TODO: Implement here the calculation of the advantage, e.g., using TD-lambda or eligibility traces\n",
    "        # Using offline forward-looking TD(lambda) with one update per episode\n",
    "        G_lam = []\n",
    "        for t in range(T):\n",
    "            acc_rew = 0\n",
    "            rew_list = []\n",
    "            for k in range(T - t):\n",
    "                acc_rew = acc_rew + self.gamma ** k * rew[t + k]\n",
    "                V_tmp = self.gamma ** (k + 1) * values[t + k + 1] if k < T - t - 1 else 0\n",
    "                rew_list.append((acc_rew + V_tmp) * self.lam ** k)\n",
    "\n",
    "            G_t = (1 - self.lam) * np.sum(rew_list[:-1]) + rew_list[-1]\n",
    "            G_lam = np.append(G_lam, G_t)\n",
    "\n",
    "            \"\"\"\n",
    "            G_n = np.zeros(T-t+1)\n",
    "            for k in range(T-t+1):\n",
    "                rew_disc = 0\n",
    "                for i in range(k):\n",
    "                    rew_disc = rew_disc + rew[t+i] * self.gamma**i\n",
    "                G_n[k] = rew_disc + values[T+k+1] * self.gamma**(k+1)\n",
    "            for i in range(t-(T-t)):\n",
    "                G_n[T-t] = G_n[T-t] + rew[t+i] * self.gamma**i\n",
    "            G_disc = 0\n",
    "            for k in range(1, T-t-1):\n",
    "                G_disc = G_disc + G_n[k-1] * self.lam**(k-1)\n",
    "            G_lam = (1-self.lam) * G_disc + G_n[T-t] * self.lam**(T-t-1)\n",
    "            \"\"\"\n",
    "\n",
    "            # Consider normalizing the advantages:\n",
    "            # TD = (TD - np.mean(TD)) / (np.std(TD) + 1e-10)\n",
    "        #G_lam = tf.convert_to_tensor(G_lam, dtype=float)\n",
    "        #print(f'G_lam: {type(G_lam)}, {len(G_lam)}, {G_lam}')\n",
    "        return G_lam\n",
    "\n",
    "    def nn_model(self, state_size, output_size, ):\n",
    "        # TODO: Define the neural network here, make sure that you account for the different requirements of the value\n",
    "        # Define NN architecture\n",
    "        input_layer = layers.Input(shape = (state_size,))\n",
    "        layer_1 = layers.Dense(64, activation = \"tanh\")(input_layer)\n",
    "        layer_2 = layers.Dense(64, activation = \"tanh\")(layer_1)\n",
    "        layer_3 = layers.Dense(64, activation = \"tanh\")(layer_2)\n",
    "        layer_4 = layers.Dense(128, activation = \"tanh\")(layer_3)\n",
    "        output_layer = layers.Dense(output_size, activation = \"tanh\")(layer_4)\n",
    "        model = keras.Model(inputs = input_layer, outputs = output_layer)\n",
    "\n",
    "        # Compile model\n",
    "        #model.compile(optimizer = \"adam\", loss = \"mse\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Here newly observed transitions are stored in the experience replay buffer\n",
    "    def record(self):  # TODO: add the relevant input arguments that you will need to store\n",
    "        return\n",
    "\n",
    "    # TODO: Define here arrays in which you will store all the information that you need in the advantage\n",
    "    #  calculation, e.g., rewards, values, states, etc.\n",
    "\n",
    "    @tf.function  # This is a wrapper that when adding it in front of a function, consisting only of tf syntax,\n",
    "    # can improve speed\n",
    "    def update_policy_parameters(self, states, actions, logprobs, G_lams, advantages):\n",
    "\n",
    "        for _ in range(self.policy_iterations):\n",
    "\n",
    "            with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "                # TODO: Use the advantages and calculated policies to calculated the clipping function here and calculate\n",
    "                #  the loss function\n",
    "                # ratio = tf.exp(policy_probabilities(self.actor.predict(states)[0], actions) - logprobs)\n",
    "                ratio = tf.exp(policy_probabilities(agent.actor(states), actions) - logprobs)\n",
    "                #print(f'values: {type(self.critic(states))}, {len(self.critic(states))}, {self.critic(states)}')\n",
    "                #print(f'G_lams2: {type(G_lams)}, {len(G_lams)}, {G_lams}')\n",
    "                #advantages = tf.subtract(G_lams, agent.critic(states))\n",
    "                ##advantages = tf.math.subtract(G_lams, 0.05)\n",
    "                #advantages returns a x*x tensor, but should return a x*0 tensor ???\n",
    "                ##advantages = list(map(lambda x, y: x - y, G_lams, tf.dtypes.cast(values_global, tf.float32)))\n",
    "                #print(f'ratio: {type(ratio)}, {len(ratio)}, {ratio}')\n",
    "                #print(f'advantages2: {type(advantages)}, {len(advantages)}, {advantages}')\n",
    "                #min_advantage = tf.where(advantages > 0, (1 + self.clip_ratio) * advantages, (1 - self.clip_ratio) * advantages,)\n",
    "                #pol_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, min_advantage))\n",
    "                clip = keras.backend.clip(ratio, min_value=1 - self.clip_ratio, max_value=1 + self.clip_ratio) * advantages\n",
    "                #print(f'clip: {type(clip)}, {len(clip)}, {clip}')\n",
    "                pol_loss = -keras.backend.mean(tf.minimum(ratio * advantages, clip))\n",
    "\n",
    "            pol_grads = tape.gradient(pol_loss, agent.actor.trainable_variables)\n",
    "            agent.actor_optimizer.apply_gradients(zip(pol_grads, agent.actor.trainable_variables))\n",
    "            #self.actor.apply_gradients(zip(pol_grads, self.actor.trainable_variables))\n",
    "\n",
    "    # This is a wrapper that when adding it in front of a function, consisting only of tf syntax,\n",
    "    # can improve speed\n",
    "    @tf.function\n",
    "    def update_value_parameters(self, G_lams, states):\n",
    "\n",
    "        for _ in range(self.value_iterations):\n",
    "\n",
    "            with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "                # TODO: Use the advantages and calculated policies to calculated the clipping function here and calculate\n",
    "                #  the loss function\n",
    "                #val_loss = tf.keras.metrics.mean_squared_error(G_lams, values_global)\n",
    "                #val_loss = tf.reduce_mean((G_lams - agent.critic(states)) ** 2)\n",
    "                val_loss = tf.keras.metrics.mean_squared_error(G_lams, agent.critic(states))\n",
    "            #print(f'len_G: {len(G_lams)}, len_values: {len(values_global)} ')\n",
    "            #print(f'len_G: {len(G_lams)}') #Problem: length of G_lam is 0 or all lengths are different\n",
    "            #print(f'values_global: {type(values_global)}, {values_global}')\n",
    "            #print(f'val_loss: {val_loss}')\n",
    "            val_grads = tape.gradient(val_loss, agent.critic.trainable_variables)\n",
    "            #print(f'val_grads: {val_grads}')\n",
    "            agent.critic_optimizer.apply_gradients(zip(val_grads, agent.critic.trainable_variables))\n",
    "            #self.critic.apply_gradients(zip(val_grads, self.critic.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 actor 0: reward: 38.0 /400\n",
      "Episode 0 actor 1: reward: 16.0 /400\n",
      "Episode 0 actor 2: reward: 19.0 /400\n",
      "Episode 0 actor 3: reward: 12.0 /400\n",
      "Episode 0 actor 4: reward: 21.0 /400\n",
      "Episode 0 actor 5: reward: 12.0 /400\n",
      "Episode 0 actor 6: reward: 18.0 /400\n",
      "Episode 0 actor 7: reward: 20.0 /400\n",
      "Episode 0 actor 8: reward: 13.0 /400\n",
      "Episode 0 actor 9: reward: 28.0 /400\n",
      "Episode 1 actor 0: reward: 37.0 /400\n",
      "Episode 1 actor 1: reward: 31.0 /400\n",
      "Episode 1 actor 2: reward: 42.0 /400\n",
      "Episode 1 actor 3: reward: 16.0 /400\n",
      "Episode 1 actor 4: reward: 29.0 /400\n",
      "Episode 1 actor 5: reward: 54.0 /400\n",
      "Episode 1 actor 6: reward: 50.0 /400\n",
      "Episode 1 actor 7: reward: 20.0 /400\n",
      "Episode 1 actor 8: reward: 20.0 /400\n",
      "Episode 1 actor 9: reward: 14.0 /400\n",
      "Episode 2 actor 0: reward: 22.0 /400\n",
      "Episode 2 actor 1: reward: 41.0 /400\n",
      "Episode 2 actor 2: reward: 45.0 /400\n",
      "Episode 2 actor 3: reward: 34.0 /400\n",
      "Episode 2 actor 4: reward: 18.0 /400\n",
      "Episode 2 actor 5: reward: 22.0 /400\n",
      "Episode 2 actor 6: reward: 39.0 /400\n",
      "Episode 2 actor 7: reward: 67.0 /400\n",
      "Episode 2 actor 8: reward: 66.0 /400\n",
      "Episode 2 actor 9: reward: 107.0 /400\n",
      "Episode 3 actor 0: reward: 67.0 /400\n",
      "Episode 3 actor 1: reward: 144.0 /400\n",
      "Episode 3 actor 2: reward: 146.0 /400\n",
      "Episode 3 actor 3: reward: 200.0 /400\n",
      "Episode 3 actor 4: reward: 87.0 /400\n",
      "Episode 3 actor 5: reward: 104.0 /400\n",
      "Episode 3 actor 6: reward: 64.0 /400\n",
      "Episode 3 actor 7: reward: 25.0 /400\n",
      "Episode 3 actor 8: reward: 169.0 /400\n",
      "Episode 3 actor 9: reward: 119.0 /400\n",
      "Episode 4 actor 0: reward: 200.0 /400\n",
      "Episode 4 actor 1: reward: 156.0 /400\n",
      "Episode 4 actor 2: reward: 169.0 /400\n",
      "Episode 4 actor 3: reward: 200.0 /400\n",
      "Episode 4 actor 4: reward: 116.0 /400\n",
      "Episode 4 actor 5: reward: 42.0 /400\n",
      "Episode 4 actor 6: reward: 117.0 /400\n",
      "Episode 4 actor 7: reward: 73.0 /400\n",
      "Episode 4 actor 8: reward: 106.0 /400\n",
      "Episode 4 actor 9: reward: 196.0 /400\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function PPO_Agent.update_policy_parameters at 0x173ad78b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function PPO_Agent.update_value_parameters at 0x16d5c48b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Episode 5 actor 0: reward: 175.0 /400\n",
      "Episode 5 actor 1: reward: 200.0 /400\n",
      "Episode 5 actor 2: reward: 154.0 /400\n",
      "Episode 5 actor 3: reward: 67.0 /400\n",
      "Episode 5 actor 4: reward: 200.0 /400\n",
      "Episode 5 actor 5: reward: 200.0 /400\n",
      "Episode 5 actor 6: reward: 171.0 /400\n",
      "Episode 5 actor 7: reward: 200.0 /400\n",
      "Episode 5 actor 8: reward: 200.0 /400\n",
      "Episode 5 actor 9: reward: 200.0 /400\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function PPO_Agent.update_policy_parameters at 0x173ad78b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function PPO_Agent.update_value_parameters at 0x16d5c48b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Episode 6 actor 0: reward: 199.0 /400\n",
      "Episode 6 actor 1: reward: 200.0 /400\n",
      "Episode 6 actor 2: reward: 149.0 /400\n",
      "Episode 6 actor 3: reward: 102.0 /400\n",
      "Episode 6 actor 4: reward: 184.0 /400\n",
      "Episode 6 actor 5: reward: 51.0 /400\n",
      "Episode 6 actor 6: reward: 108.0 /400\n",
      "Episode 6 actor 7: reward: 200.0 /400\n",
      "Episode 6 actor 8: reward: 200.0 /400\n",
      "Episode 6 actor 9: reward: 174.0 /400\n",
      "Episode 7 actor 0: reward: 200.0 /400\n",
      "Episode 7 actor 1: reward: 200.0 /400\n",
      "Episode 7 actor 2: reward: 200.0 /400\n",
      "Episode 7 actor 3: reward: 53.0 /400\n",
      "Episode 7 actor 4: reward: 134.0 /400\n",
      "Episode 7 actor 5: reward: 52.0 /400\n",
      "Episode 7 actor 6: reward: 130.0 /400\n",
      "Episode 7 actor 7: reward: 177.0 /400\n",
      "Episode 7 actor 8: reward: 173.0 /400\n",
      "Episode 7 actor 9: reward: 184.0 /400\n",
      "Episode 8 actor 0: reward: 200.0 /400\n",
      "Episode 8 actor 1: reward: 200.0 /400\n",
      "Episode 8 actor 2: reward: 200.0 /400\n",
      "Episode 8 actor 3: reward: 180.0 /400\n",
      "Episode 8 actor 4: reward: 200.0 /400\n",
      "Episode 8 actor 5: reward: 200.0 /400\n",
      "Episode 8 actor 6: reward: 200.0 /400\n",
      "Episode 8 actor 7: reward: 195.0 /400\n",
      "Episode 8 actor 8: reward: 200.0 /400\n",
      "Episode 8 actor 9: reward: 200.0 /400\n",
      "Episode 9 actor 0: reward: 200.0 /400\n",
      "Episode 9 actor 1: reward: 200.0 /400\n",
      "Episode 9 actor 2: reward: 200.0 /400\n",
      "Episode 9 actor 3: reward: 200.0 /400\n",
      "Episode 9 actor 4: reward: 200.0 /400\n",
      "Episode 9 actor 5: reward: 200.0 /400\n",
      "Episode 9 actor 6: reward: 200.0 /400\n",
      "Episode 9 actor 7: reward: 200.0 /400\n",
      "Episode 9 actor 8: reward: 200.0 /400\n",
      "Episode 9 actor 9: reward: 131.0 /400\n",
      "total reward: 12214.0 /40000\n",
      "total reward: 2346.0 /4000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    environment = GymEnvironment('CartPole-v0', 'gymresults/cartpole-v0')\n",
    "\n",
    "    no_of_states = 4  # TODO: Define number of states # [position of cart, velocity of cart, angle of pole, rotation rate of pole]\n",
    "    no_of_actions = 2  # TODO: Define number of actions # [left, right]\n",
    "\n",
    "    # The agent is initialized\n",
    "    agent = PPO_Agent(no_of_states, no_of_actions)\n",
    "\n",
    "    # Train your agent\n",
    "    no_episodes = 10  # TODO: Play around with this number, default: 500, testing: 10\n",
    "    environment.trainPPO(agent, no_episodes)\n",
    "\n",
    "    # Run your agent\n",
    "    no_episodes_run = 10 #default: 100\n",
    "    agent.actors = 1  # This is set to one here as multiple actors are only required for training\n",
    "    rew = environment.runPPO(agent, no_episodes_run)\n",
    "\n",
    "    # TODO: Implement here a function visualizing/plotting, e.g., -- NOT YET\n",
    "    # your agent's performance over the number of training episodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "machinelearning_exercises",
   "language": "python",
   "display_name": "machinelearning_exercises"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}