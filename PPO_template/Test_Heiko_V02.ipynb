{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GymEnvironment:\n",
    "    def __init__(self, env_id, monitor_dir, max_timesteps=400):\n",
    "        self.max_timesteps = max_timesteps\n",
    "\n",
    "        self.env = gym.make(env_id)\n",
    "\n",
    "    def trainPPO(self, agent, no_episodes):\n",
    "        self.runPPO(agent, no_episodes, training=True)\n",
    "\n",
    "\n",
    "    def runPPO(self, agent, no_episodes, training=False):\n",
    "\n",
    "        #rew = np.zeros(no_episodes)\n",
    "        for episode in range(no_episodes):\n",
    "            storage_size = self.actors*self.max_timesteps\n",
    "            state_storage = np.zeros((storage_size, agent.state_size), dtype=np.float32)\n",
    "            action_storage = np.zeros(storage_size, dtype=np.int32)\n",
    "            advantage_storage = np.zeros(storage_size, dtype=np.float32)\n",
    "            reward_storage = np.zeros(storage_size, dtype=np.float32)\n",
    "            return_storage = np.zeros(storage_size, dtype=np.float32)\n",
    "            value_storage = np.zeros(storage_size, dtype=np.float32)\n",
    "            logprobability_storage = np.zeros(storage_size, dtype=np.float32)\n",
    "            storage_index = 0\n",
    "            #state = self.env.reset().reshape(1, self.env.observation_space.shape[0])\n",
    "\n",
    "            for n in range(0,self.actors):\n",
    "                state = self.env.reset().reshape(1, self.env.observation_space.shape[0])\n",
    "                tot_rew = 0\n",
    "                for t in range(self.max_timesteps):\n",
    "                    # TODO: Fill out the respective to-dos in this loop and make sure that the overall algorithm works,\n",
    "                    #  e.g., overwrite current state with next state entering a new time step\n",
    "\n",
    "                    logit,action = agent.select_action(state)\n",
    "\n",
    "                    #next_state, reward, done, _ = self.env.step(action)\n",
    "                    next_state, reward, done, _ = self.env.step(action[0].numpy())\n",
    "                    next_state = next_state.reshape(1, self.env.observation_space.shape[0])\n",
    "                    tot_rew += reward\n",
    "\n",
    "\n",
    "                    if training == True:\n",
    "                    # TODO: Store relevant transition information such as rewards, values, etc. that you will need in\n",
    "                    #  the calculation of the advantages later\n",
    "                    value = agent.critic(state)\n",
    "                    logprobability = policy_probabilities(logit, action)\n",
    "                    state_storage[storage_index] = state\n",
    "                    action_storage[storage_index] = action\n",
    "                    reward_storage[storage_index] = reward\n",
    "                    value_storage[storage_index] = value\n",
    "                    logprobability_storage[storage_index] = logprobability\n",
    "                    storage_index +=1\n",
    "\n",
    "\n",
    "                    if (done == True or t == self.max_timesteps) and training == True:\n",
    "                        #TODO: Call function for calculation and storage of advantages\n",
    "                        #TODO: Store targets for your value function update\n",
    "                        break\n",
    "                rew.append(tot_rew)\n",
    "\n",
    "            #TODO: If training, call function to update policy function weights using clipping\n",
    "            #TODO: If training, Call function to update value function weights\n",
    "            # TODO: Implement here a function that evaulates the agent's performance for every x episodes by\n",
    "            # calling runDQN directly and returns an average of total rewards for 100 runs, if your objective is\n",
    "            # reached, you can terminate training\n",
    "        return rew"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def policy_probabilities(logit, a):\n",
    "        #TODO: Compute probabilities of taking actions a by using the outputs of actor NN (the logits)\n",
    "\n",
    "# Sum of discountated rewards of vectors --> useful for advantage estimates or total rewards\n",
    "def discounted_cumulative_sums(x, discount):\n",
    "        return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PPO_Agent:\n",
    "    def __init__(self, no_of_states, no_of_actions):\n",
    "        self.state_size = no_of_states\n",
    "        self.action_size = no_of_actions\n",
    "\n",
    "        #TODO: Set hyperparameters and vary them\n",
    "        self.gamma = 0.99  # discount rate\n",
    "        self.lam = 0.97 # lambda for TD(lambda)\n",
    "        self.clip_ratio = 0.01 # Clipping ratio for calculating L_clip\n",
    "        self.actors = 10 # Number of parallel actors\n",
    "\n",
    "        self.actor = self.nn_model(self.state_size,self.action_size)\n",
    "        self.critic =self.nn_model(self.state_size,1)\n",
    "\n",
    "\n",
    "    def select_action(self,state):\n",
    "        #TODO: Implement action selection, i.e., sample an action from policy pi\n",
    "        logits = self.actor(state)\n",
    "        action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "        return logits, action\n",
    "\n",
    "    def calc_advantage(self,values,rew,n,T):\n",
    "        #TODO: Implement here the calculation of the advantage, e.g., using TD-lambda or eligibility traces\n",
    "\n",
    "    def nn_model(self,state_size,output_size):\n",
    "        #TODO: Define the neural network here, make sure that you account for the different requirements of the value\n",
    "        # function and policy function approximation in the in- and outputs\n",
    "\n",
    "    # Here newly observed transitions are stored in the experience replay buffer\n",
    "    def record(self): # TODO: add the relevant input arguments that you will need to store\n",
    "        # TODO: Define here arrays in which you will store all the information that you need in the advantage\n",
    "        #  calculation, e.g., rewards, values, states, etc.\n",
    "\n",
    "    @tf.function # This is a wrapper that when adding it in front of a function, consisting only of tf syntax,\n",
    "    # can improve speed\n",
    "    def update_policy_parameters(self):\n",
    "\n",
    "        with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "            # TODO: Use the advantages and calculated policies to calculated the clipping function here and calculate\n",
    "            #  the loss function\n",
    "            pol_loss =\n",
    "\n",
    "        pol_grads = tape.gradient(pol_loss, self.actor.trainable_variables)\n",
    "        self.actor.apply_gradients(zip(pol_grads, self.actor.trainable_variables))\n",
    "\n",
    "    # This is a wrapper that when adding it in front of a function, consisting only of tf syntax,\n",
    "    # can improve speed\n",
    "    @tf.function\n",
    "    def update_value_parameters(self):\n",
    "\n",
    "        with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "            # TODO: Use the advantages and calculated policies to calculated the clipping function here and calculate\n",
    "            #  the loss function\n",
    "            val_loss = ...\n",
    "        val_grads = tape.gradient(val_loss, self.critic.trainable_variables)\n",
    "        self.critic.apply_gradients(zip(val_grads, self.critic.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    environment = GymEnvironment('CartPole-v0', 'gymresults/cartpole-v0')\n",
    "\n",
    "\n",
    "    no_of_states = #TODO: Define number of states\n",
    "    no_of_actions = #TODO: Define number of actions\n",
    "\n",
    "    # The agent is initialized\n",
    "    agent = PPO_Agent(no_of_states,no_of_actions)\n",
    "\n",
    "    # Train your agent\n",
    "    no_episodes = 500 # TODO: Play around with this number\n",
    "    environment.trainPPO(agent, no_episodes)\n",
    "\n",
    "    # Run your agent\n",
    "    no_episodes_run =\n",
    "    agent.actors = 1 # This is set to one here as multiple actors are only required for training\n",
    "    rew = environment.runPPO(agent, no_episodes_run)\n",
    "\n",
    "    # TODO: Implement here a function visualizing/plotting, e.g.,\n",
    "    # your agent's performance over the number of training episodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}