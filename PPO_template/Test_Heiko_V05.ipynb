{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import gym\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class GymEnvironment:\n",
    "    def __init__(self, env_id, monitor_dir, max_timesteps=400): #default max_timesteps = 400, testing: 10\n",
    "        self.max_timesteps = max_timesteps\n",
    "\n",
    "        self.env = gym.make(env_id)\n",
    "\n",
    "    def trainPPO(self, agent, no_episodes):\n",
    "        self.runPPO(agent, no_episodes, training=True)\n",
    "\n",
    "    def runPPO(self, agent, no_episodes, training=False):\n",
    "\n",
    "        rew = []\n",
    "        for episode in range(no_episodes):\n",
    "            states = np.zeros((agent.actors*self.max_timesteps, agent.state_size), dtype = np.float32)\n",
    "            actions = np.zeros(agent.actors*self.max_timesteps, dtype = np.int32)\n",
    "            logprobs = np.zeros(agent.actors*self.max_timesteps, dtype = np.float32)\n",
    "            advantages = []\n",
    "            G_lams = []\n",
    "            storage_counter = 0\n",
    "\n",
    "            for n in range(0, agent.actors):\n",
    "                state = self.env.reset().reshape(1, self.env.observation_space.shape[0])\n",
    "                tot_rew = 0\n",
    "                values = np.zeros(self.max_timesteps)\n",
    "                rewards = np.zeros(self.max_timesteps+1)\n",
    "                for t in range(self.max_timesteps):\n",
    "                    # TODO: Fill out the respective to-dos in this loop and make sure that the overall algorithm works,\n",
    "                    #  e.g., overwrite current state with next state entering a new time step\n",
    "\n",
    "                    logit, action = agent.select_action(state)\n",
    "                    next_state, reward, done, _ = self.env.step(action.numpy()[0])\n",
    "\n",
    "                    next_state = next_state.reshape(1, self.env.observation_space.shape[0])\n",
    "                    tot_rew += reward\n",
    "\n",
    "                    states[storage_counter] = state\n",
    "                    actions[storage_counter] = action\n",
    "\n",
    "                    \"\"\"\n",
    "                    if (done == True) or (t == self.max_timesteps-1):\n",
    "                        #checking performance:\n",
    "                        print(f'Episode {episode} actor {n}: reward: {tot_rew} /{self.max_timesteps}')\n",
    "                    \"\"\"\n",
    "\n",
    "                    #print(f'state: {type(state)}, {state}')\n",
    "                    if training == True:\n",
    "                        # TODO: Store relevant transition information such as rewards, values, etc. that you will need in\n",
    "                        #  the calculation of the advantages later\n",
    "                        rewards[t] = reward\n",
    "                        values[t] = agent.critic(state)[0]\n",
    "                        logprobs[storage_counter] = policy_probabilities(logit, action)\n",
    "\n",
    "                    state = next_state\n",
    "                    storage_counter += 1\n",
    "\n",
    "                    if ((done == True) or (t == self.max_timesteps-1)) and (training == True):\n",
    "                        # Calculate advantages when the function breaks or the last iteration is reached\n",
    "                        # TODO: Call function for calculation and storage of advantages\n",
    "                        adv, G_T = agent.calc_advantage(state, rewards, values, done, t)\n",
    "                        advantages = np.append(advantages, adv)\n",
    "                        G_lams = np.append(G_lams, G_T)\n",
    "                        print(f'Episode {episode} actor {n}: reward: {tot_rew} /{self.max_timesteps}')\n",
    "                        break\n",
    "\n",
    "                rew.append(tot_rew)\n",
    "\n",
    "            # TODO: If training, call function to update policy function weights using clipping\n",
    "            # TODO: If training, Call function to update value function weights\n",
    "            if training == True:\n",
    "                G_lams = np.array(G_lams)\n",
    "                G_lams = G_lams.astype(dtype = np.float32)\n",
    "                advantages = np.array(advantages)\n",
    "                advantages = advantages.astype(dtype = np.float32)\n",
    "                advantages = (advantages - np.mean(advantages)) / np.std(advantages)\n",
    "                states = states[0:len(G_lams)]\n",
    "                actions = actions[0:len(G_lams)]\n",
    "                logprobs = logprobs[0:len(G_lams)]\n",
    "                agent.update_policy_parameters(states,actions,logprobs,advantages)\n",
    "                agent.update_value_parameters(G_lams, states)\n",
    "                # TODO: Implement here a function that evaulates the agent's performance for every x episodes by\n",
    "                #  calling PPO directly and returns an average of total rewards for 100 runs, if your objective is\n",
    "                #  reached, you can terminate training\n",
    "\n",
    "        print(f'total reward: {sum(rew)} /{self.max_timesteps*agent.actors*no_episodes}')\n",
    "        return rew"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def policy_probabilities(logit, action):\n",
    "    logprobs = tf.nn.log_softmax(logit)\n",
    "    logprob = tf.reduce_sum(tf.one_hot(action, 2) * logprobs, axis=1)\n",
    "    return logprob\n",
    "\n",
    "\n",
    "# Sum of discountated rewards of vectors --> useful for advantage estimates or total rewards\n",
    "def discounted_cumulative_sums(x, discount):\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class PPO_Agent:\n",
    "    def __init__(self, no_of_states, no_of_actions):\n",
    "        self.state_size = no_of_states\n",
    "        self.action_size = no_of_actions\n",
    "\n",
    "        # TODO: Set hyperparameters and vary them\n",
    "        self.gamma = 0.99  # discount rate, 0.9\n",
    "        self.lam = 0.97  # lambda for TD(lambda), 0.6\n",
    "        self.clip_ratio = 0.2  # Clipping ratio for calculating L_clip, 0.5\n",
    "        self.lr = 0.0003  # learning rate, default: 0.0001\n",
    "        self.actors = 10  # Number of parallel actors, default: 100, testing: 10\n",
    "\n",
    "        self.policy_iterations = 20 # number of policy updates per update call\n",
    "        self.value_iterations = 20 # number of value updates per update call\n",
    "\n",
    "        self.actor = self.nn_model(self.state_size, self.action_size)\n",
    "        self.actor_optimizer = keras.optimizers.Adam(learning_rate=self.lr)\n",
    "        self.critic = self.nn_model(self.state_size, 1)\n",
    "        self.critic_optimizer = keras.optimizers.Adam(learning_rate=self.lr)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # TODO: Implement action selection, i.e., sample an action from policy pi\n",
    "        logit = agent.actor(state)\n",
    "        action = tf.squeeze(tf.random.categorical(logit, 1), axis=1)\n",
    "        return logit, action\n",
    "\n",
    "    def calc_advantage(self, state, rewards, values, done, t):\n",
    "        # TODO: Implement here the calculation of the advantage, e.g., using TD-lambda or eligibility traces\n",
    "        # Using offline forward-looking TD(lambda) with one update per episode\n",
    "\n",
    "        rewards = rewards[:t+1]\n",
    "        values = values[:t+1]\n",
    "        if done == True:\n",
    "            values = np.array(np.append(values, 0))\n",
    "        else:\n",
    "            values = np.array(np.append(values, agent.critic(state)[0]))\n",
    "        TD_delta = rewards + agent.gamma * values[1:] - values[:-1]\n",
    "        adv = discounted_cumulative_sums(TD_delta, agent.gamma*agent.lam)\n",
    "\n",
    "        if done == False:\n",
    "            rewards = np.array(np.append(rewards, agent.critic(state)[0]))\n",
    "            G_T = discounted_cumulative_sums(rewards, agent.gamma)[:-1] #G_t\n",
    "        else:\n",
    "            G_T = discounted_cumulative_sums(rewards, agent.gamma) #G_t\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        rewards = rewards[:t+1]\n",
    "        values = values[:t+1]\n",
    "        T_deltas = rewards + agent.gamma * values - values\n",
    "        adv = discounted_cumulative_sums(T_deltas, agent.gamma*agent.lam)\n",
    "        if done == True:\n",
    "            rewards = np.array(np.append(rewards, 0))\n",
    "        else:\n",
    "            rewards = np.array(np.append(rewards, agent.critic(state)[0]))\n",
    "        G_T = discounted_cumulative_sums(rewards, agent.gamma)[:-1]\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        G_lam = []\n",
    "        for t in range(T):\n",
    "            acc_rew = 0\n",
    "            rew_list = []\n",
    "            for k in range(T - t):\n",
    "                acc_rew = acc_rew + self.gamma ** k * rew[t + k]\n",
    "                V_tmp = self.gamma ** (k + 1) * values[t + k + 1] if k < T - t - 1 else 0\n",
    "                rew_list.append((acc_rew + V_tmp) * self.lam ** k)\n",
    "\n",
    "            G_t = (1 - self.lam) * np.sum(rew_list[:-1]) + rew_list[-1]\n",
    "            G_lam = np.append(G_lam, G_t)\n",
    "\n",
    "            # Consider normalizing the advantages:\n",
    "            # TD = (TD - np.mean(TD)) / (np.std(TD) + 1e-10)\n",
    "        #G_lam = tf.convert_to_tensor(G_lam, dtype=float)\n",
    "        #print(f'G_lam: {type(G_lam)}, {len(G_lam)}, {G_lam}')\n",
    "        return G_lam\n",
    "        \"\"\"\n",
    "\n",
    "        return adv, G_T\n",
    "\n",
    "    def nn_model(self, state_size, output_size, ):\n",
    "        # TODO: Define the neural network here, make sure that you account for the different requirements of the value\n",
    "        input_layer = layers.Input(shape = (state_size,))\n",
    "        layer_1 = layers.Dense(64, activation = \"tanh\")(input_layer)\n",
    "        layer_2 = layers.Dense(64, activation = \"tanh\")(layer_1)\n",
    "        layer_3 = layers.Dense(64, activation = \"tanh\")(layer_2)\n",
    "        layer_4 = layers.Dense(128, activation = \"tanh\")(layer_3)\n",
    "        output_layer = layers.Dense(output_size, activation = \"tanh\")(layer_4)\n",
    "        model = keras.Model(inputs = input_layer, outputs = output_layer)\n",
    "\n",
    "        return model\n",
    "\n",
    "    #Delete?\n",
    "    \"\"\"\n",
    "    # Here newly observed transitions are stored in the experience replay buffer\n",
    "    def record(self):  # TODO: add the relevant input arguments that you will need to store\n",
    "        return\n",
    "\n",
    "    # TODO: Define here arrays in which you will store all the information that you need in the advantage\n",
    "    #  calculation, e.g., rewards, values, states, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    @tf.function  # This is a wrapper that when adding it in front of a function, consisting only of tf syntax,\n",
    "    # can improve speed\n",
    "    def update_policy_parameters(self, states, actions, logprobs, advantages):\n",
    "\n",
    "        for _ in range(self.policy_iterations):\n",
    "\n",
    "            with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "                # TODO: Use the advantages and calculated policies to calculated the clipping function here and calculate\n",
    "                #  the loss function\n",
    "                ratio = tf.exp(policy_probabilities(agent.actor(states), actions) - logprobs)\n",
    "                #min_advantage = tf.where(advantages > 0, (1 + self.clip_ratio) * advantages, (1 - self.clip_ratio) * advantages,)\n",
    "                #pol_loss = -tf.reduce_mean(tf.minimum(ratio * advantages, min_advantage))\n",
    "                clip = keras.backend.clip(ratio, min_value=1 - self.clip_ratio, max_value=1 + self.clip_ratio) * advantages\n",
    "                pol_loss = -keras.backend.mean(tf.minimum(ratio * advantages, clip))\n",
    "\n",
    "            pol_grads = tape.gradient(pol_loss, agent.actor.trainable_variables)\n",
    "            agent.actor_optimizer.apply_gradients(zip(pol_grads, agent.actor.trainable_variables))\n",
    "\n",
    "    # This is a wrapper that when adding it in front of a function, consisting only of tf syntax,\n",
    "    # can improve speed\n",
    "    @tf.function\n",
    "    def update_value_parameters(self, G_lams, states):\n",
    "\n",
    "        for _ in range(self.value_iterations):\n",
    "\n",
    "            with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "                # TODO: Use the advantages and calculated policies to calculated the clipping function here and calculate\n",
    "                #  the loss function\n",
    "                #val_loss = tf.reduce_mean((G_lams - agent.critic(states)) ** 2)\n",
    "                val_loss = tf.keras.metrics.mean_squared_error(G_lams, agent.critic(states))\n",
    "            val_grads = tape.gradient(val_loss, agent.critic.trainable_variables)\n",
    "            agent.critic_optimizer.apply_gradients(zip(val_grads, agent.critic.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 actor 0: reward: 13.0 /400\n",
      "Episode 0 actor 1: reward: 39.0 /400\n",
      "Episode 0 actor 2: reward: 25.0 /400\n",
      "Episode 0 actor 3: reward: 31.0 /400\n",
      "Episode 0 actor 4: reward: 31.0 /400\n",
      "Episode 0 actor 5: reward: 13.0 /400\n",
      "Episode 0 actor 6: reward: 16.0 /400\n",
      "Episode 0 actor 7: reward: 14.0 /400\n",
      "Episode 0 actor 8: reward: 19.0 /400\n",
      "Episode 0 actor 9: reward: 20.0 /400\n",
      "Episode 1 actor 0: reward: 35.0 /400\n",
      "Episode 1 actor 1: reward: 53.0 /400\n",
      "Episode 1 actor 2: reward: 37.0 /400\n",
      "Episode 1 actor 3: reward: 17.0 /400\n",
      "Episode 1 actor 4: reward: 20.0 /400\n",
      "Episode 1 actor 5: reward: 19.0 /400\n",
      "Episode 1 actor 6: reward: 24.0 /400\n",
      "Episode 1 actor 7: reward: 23.0 /400\n",
      "Episode 1 actor 8: reward: 22.0 /400\n",
      "Episode 1 actor 9: reward: 19.0 /400\n",
      "Episode 2 actor 0: reward: 14.0 /400\n",
      "Episode 2 actor 1: reward: 27.0 /400\n",
      "Episode 2 actor 2: reward: 50.0 /400\n",
      "Episode 2 actor 3: reward: 15.0 /400\n",
      "Episode 2 actor 4: reward: 33.0 /400\n",
      "Episode 2 actor 5: reward: 20.0 /400\n",
      "Episode 2 actor 6: reward: 27.0 /400\n",
      "Episode 2 actor 7: reward: 18.0 /400\n",
      "Episode 2 actor 8: reward: 50.0 /400\n",
      "Episode 2 actor 9: reward: 24.0 /400\n",
      "Episode 3 actor 0: reward: 39.0 /400\n",
      "Episode 3 actor 1: reward: 14.0 /400\n",
      "Episode 3 actor 2: reward: 10.0 /400\n",
      "Episode 3 actor 3: reward: 76.0 /400\n",
      "Episode 3 actor 4: reward: 35.0 /400\n",
      "Episode 3 actor 5: reward: 48.0 /400\n",
      "Episode 3 actor 6: reward: 49.0 /400\n",
      "Episode 3 actor 7: reward: 49.0 /400\n",
      "Episode 3 actor 8: reward: 30.0 /400\n",
      "Episode 3 actor 9: reward: 15.0 /400\n",
      "Episode 4 actor 0: reward: 14.0 /400\n",
      "Episode 4 actor 1: reward: 41.0 /400\n",
      "Episode 4 actor 2: reward: 48.0 /400\n",
      "Episode 4 actor 3: reward: 69.0 /400\n",
      "Episode 4 actor 4: reward: 77.0 /400\n",
      "Episode 4 actor 5: reward: 65.0 /400\n",
      "Episode 4 actor 6: reward: 18.0 /400\n",
      "Episode 4 actor 7: reward: 91.0 /400\n",
      "Episode 4 actor 8: reward: 48.0 /400\n",
      "Episode 4 actor 9: reward: 52.0 /400\n",
      "Episode 5 actor 0: reward: 71.0 /400\n",
      "Episode 5 actor 1: reward: 15.0 /400\n",
      "Episode 5 actor 2: reward: 73.0 /400\n",
      "Episode 5 actor 3: reward: 34.0 /400\n",
      "Episode 5 actor 4: reward: 31.0 /400\n",
      "Episode 5 actor 5: reward: 46.0 /400\n",
      "Episode 5 actor 6: reward: 81.0 /400\n",
      "Episode 5 actor 7: reward: 69.0 /400\n",
      "Episode 5 actor 8: reward: 103.0 /400\n",
      "Episode 5 actor 9: reward: 30.0 /400\n",
      "Episode 6 actor 0: reward: 200.0 /400\n",
      "Episode 6 actor 1: reward: 129.0 /400\n",
      "Episode 6 actor 2: reward: 117.0 /400\n",
      "Episode 6 actor 3: reward: 200.0 /400\n",
      "Episode 6 actor 4: reward: 155.0 /400\n",
      "Episode 6 actor 5: reward: 191.0 /400\n",
      "Episode 6 actor 6: reward: 200.0 /400\n",
      "Episode 6 actor 7: reward: 77.0 /400\n",
      "Episode 6 actor 8: reward: 181.0 /400\n",
      "Episode 6 actor 9: reward: 108.0 /400\n",
      "Episode 7 actor 0: reward: 200.0 /400\n",
      "Episode 7 actor 1: reward: 200.0 /400\n",
      "Episode 7 actor 2: reward: 200.0 /400\n",
      "Episode 7 actor 3: reward: 112.0 /400\n",
      "Episode 7 actor 4: reward: 151.0 /400\n",
      "Episode 7 actor 5: reward: 96.0 /400\n",
      "Episode 7 actor 6: reward: 200.0 /400\n",
      "Episode 7 actor 7: reward: 200.0 /400\n",
      "Episode 7 actor 8: reward: 13.0 /400\n",
      "Episode 7 actor 9: reward: 200.0 /400\n",
      "Episode 8 actor 0: reward: 200.0 /400\n",
      "Episode 8 actor 1: reward: 200.0 /400\n",
      "Episode 8 actor 2: reward: 200.0 /400\n",
      "Episode 8 actor 3: reward: 200.0 /400\n",
      "Episode 8 actor 4: reward: 200.0 /400\n",
      "Episode 8 actor 5: reward: 200.0 /400\n",
      "Episode 8 actor 6: reward: 200.0 /400\n",
      "Episode 8 actor 7: reward: 132.0 /400\n",
      "Episode 8 actor 8: reward: 200.0 /400\n",
      "Episode 8 actor 9: reward: 200.0 /400\n",
      "Episode 9 actor 0: reward: 200.0 /400\n",
      "Episode 9 actor 1: reward: 79.0 /400\n",
      "Episode 9 actor 2: reward: 200.0 /400\n",
      "Episode 9 actor 3: reward: 200.0 /400\n",
      "Episode 9 actor 4: reward: 153.0 /400\n",
      "Episode 9 actor 5: reward: 200.0 /400\n",
      "Episode 9 actor 6: reward: 200.0 /400\n",
      "Episode 9 actor 7: reward: 200.0 /400\n",
      "Episode 9 actor 8: reward: 200.0 /400\n",
      "Episode 9 actor 9: reward: 200.0 /400\n",
      "total reward: 9103.0 /40000\n",
      "total reward: 2465.0 /4000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    environment = GymEnvironment('CartPole-v0', 'gymresults/cartpole-v0')\n",
    "\n",
    "    no_of_states = 4  # TODO: Define number of states # [position of cart, velocity of cart, angle of pole, rotation rate of pole]\n",
    "    no_of_actions = 2  # TODO: Define number of actions # [left, right]\n",
    "\n",
    "    # The agent is initialized\n",
    "    agent = PPO_Agent(no_of_states, no_of_actions)\n",
    "\n",
    "    # Train your agent\n",
    "    no_episodes = 10  # TODO: Play around with this number, default: 500, testing: 10\n",
    "    environment.trainPPO(agent, no_episodes)\n",
    "\n",
    "    # Run your agent\n",
    "    no_episodes_run = 10 #default: 100\n",
    "    agent.actors = 1  # This is set to one here as multiple actors are only required for training\n",
    "    rew = environment.runPPO(agent, no_episodes_run)\n",
    "\n",
    "    # TODO: Implement here a function visualizing/plotting, e.g., -- NOT YET\n",
    "    # your agent's performance over the number of training episodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "[4.70886581 3.823573   2.9109     1.97       1.        ]\n"
     ]
    }
   ],
   "source": [
    "rew = np.ones(5)\n",
    "lam = 0.97\n",
    "print(rew)\n",
    "print(scipy.signal.lfilter([1], [1, float(-lam)], rew[::-1], axis=0)[::-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "machinelearning_exercises",
   "language": "python",
   "display_name": "machinelearning_exercises"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}